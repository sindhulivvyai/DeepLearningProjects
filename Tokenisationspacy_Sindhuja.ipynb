{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1elWBPh10cRe9xq_dKJ13E88VMJT3J0rA","timestamp":1753263743487}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PC3FEGazUErw","executionInfo":{"status":"ok","timestamp":1753263867867,"user_tz":-330,"elapsed":33528,"user":{"displayName":"Sindhuja S","userId":"17199322313890370252"}},"outputId":"f420b4ba-3db6-4173-a9f3-88fce339294b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","\n","📌 Alternative: Padded Tokenized Sentences (spaCy only):\n","\n","['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>']\n","\n","📘 Alternative Vocabulary (token → index):\n","\n","0: <unk>\n","1: <pad>\n","2: <bos>\n","3: <eos>\n","4: IBM\n","5: taught\n","6: me\n","7: tokenization\n","8: Special\n","9: tokenizers\n","10: are\n","11: ready\n","12: and\n","13: they\n","14: will\n","15: blow\n","16: your\n","17: mind\n","\n","🔢 Alternative: Tokenized Sentences as Indices:\n","\n","Sentence 1: [2, 4, 5, 6, 7, 3, 1, 1, 1, 1, 1, 1]\n","Sentence 2: [2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 3]\n"]}],"source":["# ======================================\n","# ✅ WORKING SOLUTION: spaCy-only Tokenization (No torchtext needed)\n","# ======================================\n","\n","# STEP 1: INSTALL ONLY SPACY (avoid torchtext dependency conflicts)\n","!pip install -U spacy -q\n","!python -m spacy download en_core_web_sm\n","\n","# 🔁 After installation, restart runtime and run the code below\n","\n","# ======================================\n","# MAIN SOLUTION: Using spaCy directly\n","# ======================================\n","\n","import spacy\n","\n","# Load spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Define sample text data\n","lines = [\n","    \"IBM taught me tokenization\",\n","    \"Special tokenizers are ready and they will blow your mind\"\n","]\n","\n","# Tokenize using spaCy directly\n","tokens_alt = []\n","max_length_alt = 0\n","\n","for line in lines:\n","    doc = nlp(line)\n","    tokenized_line = [token.text for token in doc]  # Extract token text\n","    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n","    tokens_alt.append(tokenized_line)\n","    max_length_alt = max(max_length_alt, len(tokenized_line))\n","\n","# Pad sentences\n","for i in range(len(tokens_alt)):\n","    tokens_alt[i] += ['<pad>'] * (max_length_alt - len(tokens_alt[i]))\n","\n","print(\"\\n📌 Alternative: Padded Tokenized Sentences (spaCy only):\\n\")\n","for sentence in tokens_alt:\n","    print(sentence)\n","\n","# Build vocabulary manually\n","from collections import Counter\n","\n","# Collect all tokens\n","all_tokens = [token for sentence in tokens_alt for token in sentence]\n","token_counts = Counter(all_tokens)\n","\n","# Create vocabulary dictionary\n","vocab_alt = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n","for token, count in token_counts.items():\n","    if token not in vocab_alt:\n","        vocab_alt[token] = len(vocab_alt)\n","\n","print(\"\\n📘 Alternative Vocabulary (token → index):\\n\")\n","for token, idx in vocab_alt.items():\n","    print(f\"{idx}: {token}\")\n","\n","# Convert to indices\n","indexed_sentences_alt = [[vocab_alt[token] for token in sentence] for sentence in tokens_alt]\n","\n","print(\"\\n🔢 Alternative: Tokenized Sentences as Indices:\\n\")\n","for i, indices in enumerate(indexed_sentences_alt):\n","    print(f\"Sentence {i+1}: {indices}\")"]}]}